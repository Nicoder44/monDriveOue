<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>M&eacute;mo sur la Classification Non Supervis&eacute;e avec les K-Means et le Gaussian Mixture Model</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <h1 id="mémo-sur-la-classification-non-supervisée-avec-les-k-means-et-le-gaussian-mixture-model">Mémo sur la Classification Non Supervisée avec les K-Means et le Gaussian Mixture Model</h1>
<h2 id="1-classification-non-supervisée">1. Classification Non Supervisée</h2>
<p>La classification non supervisée vise à regrouper automatiquement des données similaires sans connaître leurs étiquettes. Deux techniques courantes sont les K-Means et le Gaussian Mixture Model (GMM).</p>
<h2 id="2-k-means">2. K-Means</h2>
<h3 id="algorithme">Algorithme</h3>
<p>L'algorithme K-Means cherche à minimiser l'inertie ou la variance intra-classe pour un nombre de classes ( k ) fixé. La formule de l'inertie ( E ) est donnée par :</p>
<p>[ E = \sum_{i=1}^{K} \sum_{x_j \in C_i} d(x_j, \bar{x}_i) ]</p>
<ul>
<li>( C_i ) : Cluster ( i )</li>
<li>( \bar{x}_i ) : Centre du cluster ( i )</li>
<li>( d(.,.) ) : Distance entre deux points (par exemple, distance euclidienne)</li>
</ul>
<h3 id="exemples-de-questions">Exemples de Questions</h3>
<ul>
<li>Que vaut ( E ) pour différentes partitions ( C_1 = (A, C, E, F) ) et ( C_2 = (B, D, G) ) ?</li>
<li>Quelle est la meilleure partition ? (Utilisation de [sklearn.cluster.KMeans])</li>
<li>Quels sont les clusters pour un nombre donné ( k ) de catégories ?</li>
<li>Faire le moyenne de chaque cluster, puis parcourir tous les points et faire la différence entre le point et la moyenne de son cluster. E vaut somme de abs(point - moy cluster)</li>
<li>On cherche a minimiser E</li>
</ul>
<h3 id="application-aux-images">Application aux Images</h3>
<ul>
<li>Regroupement d'images en catégories basé sur la similarité.</li>
</ul>
<h2 id="3-gaussian-mixture-model-gmm">3. Gaussian Mixture Model (GMM)</h2>
<p>Le GMM modélise chaque cluster par une densité de probabilité (loi normale) en supposant que les données peuvent être représentées comme une somme pondérée de K gaussiennes.</p>
<h3 id="formule-de-probabilité">Formule de Probabilité</h3>
<p>[ p(x) = \sum_{i=1}^{K} \pi_i \cdot p_{\mu_i, \sigma_i}(x) ]</p>
<ul>
<li>( \pi_i ) : Proportion de la classe ( i )</li>
<li>( p_{\mu_i, \sigma_i}(x) ) : Densité de probabilité gaussienne</li>
</ul>
<h3 id="exemples-de-questions-1">Exemples de Questions</h3>
<ul>
<li>Quels sont les regroupements pour une hypothèse de 3 groupes ?</li>
<li>Quelle est la proportion de chaque classe dans le mélange ?</li>
<li>Quelles sont les propriétés (moyenne, matrice de variance-covariance) de la loi normale associée à un groupe spécifique ?</li>
</ul>
<h3 id="application-aux-images-1">Application aux Images</h3>
<ul>
<li>Regroupement d'images en catégories basé sur le modèle de mélange gaussien.</li>
</ul>
<hr>
<p><strong>Remarque :</strong> Ces techniques sont utilisées pour la classification non supervisée, où les données ne sont pas étiquetées, et permettent de découvrir des structures intrinsèques dans les ensembles de données.</p>
<pre><code>Clusters = [[1, 3, 2], [5, 7, 6, 8]]

means = []
E = 0
for Cluster in Clusters:
    mean_c = 0
    for c in Cluster:
        mean_c += c
    mean_c /= Cluster.__len__()
    means.append(mean_c)
idx = 0
for Cluster in Clusters:
    for c in Cluster:
        E += abs(means[idx] - c)
    idx += 1
print(&quot;Final E for the Clusters: &quot; + str(E))

#With sklearn.cluster.KMeans
print(&quot;ex2-------------------------------------------------------------------&quot;)
################################################################################
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
#scaler = StandardScaler(copy=True, with_mean=True, with_std=True)

dots = np.array([[1, 7, 3, 6, 2, 5, 8]]).transpose()

#data = scaler.fit_transform(dots)

kmeans = KMeans(n_clusters=2, n_init=10)
kmeans.fit(dots)
#print(&quot;Identifiants: &quot;, ids)
print(&quot;Data : &quot;, dots)
print(&quot;Categories : &quot;, kmeans.labels_)
print(&quot;ex3-------------------------------------------------------------------&quot;)
</code></pre>

        
        
    </body>
    </html>